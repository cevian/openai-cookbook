{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search with time-based filtering using LlamaIndex\n",
    "\n",
    "A key use case for Timescale Vector is efficient time-based vector search. Timescale Vector enables this by automatically partitioning vectors (and associated metadata) by time. This allows you to efficiently query vectors by both similarity to a query vector and time because we can exclude entire partitions that don't overlap with the query time.\n",
    "\n",
    "Time-based vector search functionality is helpful for applications like:\n",
    "- Storing and retrieving LLM response history (e.g. chatbots)\n",
    "- Finding the most recent embeddings that are similar to a query vector (e.g recent news).\n",
    "- Constraining similarity search to a relevant time range (e.g asking time-based questions about a knowledge base)\n",
    "\n",
    "To illustrate how to use TimescaleVector's time-based vector search functionality, we'll ask questions about the git log history for TimescaleDB . We'll illustrate how to add documents with a time-based uuid and how run similarity searches with time range filters.\n",
    "\n",
    "## Setup your environment\n",
    "\n",
    "First, install the necessary prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pip install necessary packages\n",
    "%pip install -U timescale-vector\n",
    "%pip install -U openai\n",
    "%pip install -U llama_index\n",
    "%pip install -U llama-index-readers-json\n",
    "%pip install -U llama-index-embeddings-openai\n",
    "%pip install -U llama-index-vector-stores-timescalevector\n",
    "%pip install -U python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, setup your secrets. You'll need an API key from [OpenAI](https://platform.openai.com/) and a service url from [Timescale](https://console.cloud.timescale.com/signup?utm_campaign=vectorlaunch&utm_source=aicookbooks&utm_medium=referral). For security, we suggest storing these in a dotenv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get openAI api key by reading local .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv(), override=True)\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "TIMESCALE_SERVICE_URL = os.environ[\"TIMESCALE_SERVICE_URL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract content and metadata from git log JSON\n",
    "First lets load in the git log data into a new collection in our PostgreSQL database named `timescale_commits`\n",
    "\n",
    "You'll need to [download the sample dataset](https://s3.amazonaws.com/assets.timescale.com/ai/ts_git_log.json) and place it in the same directory as this notebook.\n",
    "\n",
    "You can use following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file using curl and save it as commit_history.csv\n",
    "# Note: Execute this command in your terminal, in the same directory as the notebook\n",
    "!curl -O \"https://s3.amazonaws.com/assets.timescale.com/ai/commit_history.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a set of functions to extract metadata from the git commits. Note, how we create the UUID based on the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timescale_vector import client\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_date(date_string: str) -> datetime:\n",
    "    if date_string is None:\n",
    "        return None\n",
    "    time_format = \"%a %b %d %H:%M:%S %Y %z\"\n",
    "    return datetime.strptime(date_string, time_format)\n",
    "\n",
    "# Function to take in a date string in the past and return a uuid v1\n",
    "def create_uuid(date_string: str):\n",
    "   datetime_obj = parse_date(date_string)\n",
    "   uuid = client.uuid_from_time(datetime_obj)\n",
    "   return str(uuid)\n",
    "\n",
    "def split_name(input_string: str):\n",
    "    if input_string is None:\n",
    "        return None, None\n",
    "    start = input_string.find(\"<\")\n",
    "    end = input_string.find(\">\")\n",
    "    name = input_string[:start].strip()\n",
    "    email = input_string[start + 1 : end].strip()\n",
    "    return name, email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we loade the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from llama_index.core.schema import TextNode\n",
    "# Create a Node object from a single row of data\n",
    "def create_node(row):\n",
    "   record = row.to_dict()\n",
    "   (record_name, _)= split_name(record[\"author\"])\n",
    "   record_content = str(record[\"date\"]) + \" \" + record_name + \" \" + str(record[\"change summary\"]) + \" \" + str(record[\"change details\"])\n",
    "   node = TextNode(\n",
    "       id_=create_uuid(record[\"date\"]),\n",
    "       text= record_content,\n",
    "       metadata={\n",
    "           'commit': record[\"commit\"],\n",
    "           'author': record_name,\n",
    "           'date': str(parse_date(record[\"date\"])),\n",
    "       }\n",
    "   )\n",
    "   return node\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "file_path = Path(\"commit_history.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.dropna()\n",
    "nodes = [create_node(row) for _, row in df.iterrows()]\n",
    "\n",
    "NUM_RECORDS = 20\n",
    "nodes = nodes[:NUM_RECORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a demo, we will only load the first 500 records. In practice, you can load as many records as you want.\n",
    "\n",
    "Next, we will create embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for nodes\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "embedding_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "for node in nodes:\n",
    "   node_embedding = embedding_model.get_text_embedding(\n",
    "       node.get_content(metadata_mode=\"all\")\n",
    "   )\n",
    "   node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents and metadata into TimescaleVector vectorstore\n",
    "Now that we have prepared our documents, let's process them and load them, along with their vector embedding representations into our Timescale Vector.\n",
    "\n",
    "First, we'll define a table name for the data.\n",
    "\n",
    "We'll also define a time delta, which we pass to the `time_partition_interval` argument, which will be used to as the interval for partitioning the data by time. Each partition will consist of data for the specified length of time. We'll use 7 days for simplicity, but you can pick whatever value make sense for your use case -- for example if you query recent vectors frequently you might want to use a smaller time delta like 1 day, or if you query vectors over a decade long time period then you might want to use a larger time delta like 6 months or 1 year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.timescalevector import TimescaleVectorStore\n",
    "from datetime import timedelta\n",
    "\n",
    "# Create a timescale vector store and add the newly created nodes to it\n",
    "ts_vector_store = TimescaleVectorStore.from_params(\n",
    "   service_url=TIMESCALE_SERVICE_URL,\n",
    "   table_name=\"li_commit_history\",\n",
    "   time_partition_interval= timedelta(days=7),\n",
    ")\n",
    "ts_vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an index to speed up queries. Using the `create_index()` function without additional arguments will create a timescale_vector_index by default, using the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index\n",
    "# by default this will create a Timescale Vector (DiskANN) index\n",
    "ts_vector_store.create_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying vectors by time and similarity\n",
    "\n",
    "Now that we have loaded our documents into TimescaleVector, we can query them by time and similarity.\n",
    "\n",
    "TimescaleVector does this effeciently because any partition (7-day period in this example) that does not overlap with the query is completely excluded.\n",
    "\n",
    "TimescaleVector provides multiple methods for querying vectors by doing similarity search with time-based filtering. \n",
    "\n",
    "Let's take a look at each method below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time filter variables\n",
    "start_dt = datetime(2023, 8, 1, 22, 10, 35)  # Start date = 1 August 2023, 22:10:35\n",
    "end_dt = datetime(2023, 8, 30, 22, 10, 35)  # End date = 30 August 2023, 22:10:35\n",
    "td = timedelta(days=7)  # Time delta = 7 days\n",
    "\n",
    "query = \"What's new with TimescaleDB functions?\"\n",
    "query_embedding = embedding_model.get_query_embedding(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Filter within a provided start date and end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=5\n",
    ")\n",
    "\n",
    "query_result = ts_vector_store.query(\n",
    "    vector_store_query, start_date=start_dt, end_date=end_dt\n",
    ")\n",
    "\n",
    "for node in query_result.nodes:\n",
    "    print(\"-\" * 80)\n",
    "    print(node.metadata[\"date\"])\n",
    "    print(node.get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Filter within a provided start date, and a time delta later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=5\n",
    ")\n",
    "\n",
    "query_result = ts_vector_store.query(\n",
    "    vector_store_query, start_date=start_dt, timedelta=td\n",
    ")\n",
    "\n",
    "for node in query_result.nodes:\n",
    "    print(\"-\" * 80)\n",
    "    print(node.metadata[\"date\"])\n",
    "    print(node.get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3: Filter within a provided end date and a time delta earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=5\n",
    ")\n",
    "\n",
    "query_result = ts_vector_store.query(\n",
    "    vector_store_query, end_date=end_dt, timedelta=td\n",
    ")\n",
    "\n",
    "for node in query_result.nodes:\n",
    "    print(\"-\" * 80)\n",
    "    print(node.metadata[\"date\"])\n",
    "    print(node.get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying using other metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also filter based on other metadata.  This shows a similarity search with a filter on author name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import VectorStoreQuery, MetadataFilters, MetadataFilter, FilterOperator\n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(\n",
    "            key=\"author\", operator=FilterOperator.EQ, value=\"Sven Klemm\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=5, filters=filters,\n",
    ")\n",
    "\n",
    "query_result = ts_vector_store.query(\n",
    "    vector_store_query\n",
    ")\n",
    "\n",
    "for node in query_result.nodes:\n",
    "    print(\"-\" * 80)\n",
    "    print(node.metadata[\"date\"])\n",
    "    print(node.get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also combine it with time-based search. This shows a similarity search with a time filter as well as a filter on author name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import VectorStoreQuery, MetadataFilters, MetadataFilter, FilterOperator\n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(\n",
    "            key=\"author\", operator=FilterOperator.EQ, value=\"Sven Klemm\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=5, filters=filters,\n",
    ")\n",
    "\n",
    "query_result = ts_vector_store.query(\n",
    "    vector_store_query, start_date=start_dt, end_date=end_dt\n",
    ")\n",
    "\n",
    "for node in query_result.nodes:\n",
    "    print(\"-\" * 80)\n",
    "    print(node.metadata[\"date\"])\n",
    "    print(node.get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
