{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO3BxLfwl0Vh"
      },
      "source": [
        "# Similarity Search with time-based filtering using LangChain\n",
        "\n",
        "A key use case for Timescale Vector is efficient time-based vector search. Timescale Vector enables this by automatically partitioning vectors (and associated metadata) by time. This allows you to efficiently query vectors by both similarity to a query vector and time because we can exclude entire partitions that don't overlap with the query time.\n",
        "\n",
        "Time-based vector search functionality is helpful for applications like:\n",
        "- Storing and retrieving LLM response history (e.g. chatbots)\n",
        "- Finding the most recent embeddings that are similar to a query vector (e.g recent news).\n",
        "- Constraining similarity search to a relevant time range (e.g asking time-based questions about a knowledge base)\n",
        "\n",
        "To illustrate how to use TimescaleVector's time-based vector search functionality, we'll ask questions about the git log history for TimescaleDB . We'll illustrate how to add documents with a time-based uuid and how run similarity searches with time range filters.\n",
        "\n",
        "## Setup your environment\n",
        "\n",
        "First, install the necessary prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir9d_rlVl0Vj",
        "outputId": "2e2d5c4b-fa13-4642-90c0-4319ac526d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timescale-vector in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: asyncpg in /usr/local/lib/python3.10/dist-packages (from timescale-vector) (0.29.0)\n",
            "Requirement already satisfied: psycopg2 in /usr/local/lib/python3.10/dist-packages (from timescale-vector) (2.9.9)\n",
            "Requirement already satisfied: pgvector in /usr/local/lib/python3.10/dist-packages (from timescale-vector) (0.2.5)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from asyncpg->timescale-vector) (4.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pgvector->timescale-vector) (1.25.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.14.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.13)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.29 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.29)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.33)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.31)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: jq in /usr/local/lib/python3.10/dist-packages (1.7.0)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "# Pip install necessary packages\n",
        "%pip install timescale-vector\n",
        "%pip install openai\n",
        "%pip install langchain\n",
        "%pip install python-dotenv\n",
        "%pip install jq\n",
        "%pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmdRgrNal0Vk"
      },
      "source": [
        "Next, setup your secrets. You'll need an API key from [OpenAI](https://platform.openai.com/) and a service url from [Timescale](https://console.cloud.timescale.com/signup?utm_campaign=vectorlaunch&utm_source=aicookbooks&utm_medium=referral).\n",
        "\n",
        "For security, we suggest storing these in a dotenv file if running locally (see below for getting secrets in Google Colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCgihQWOl0Vk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get openAI api key by reading local .env file\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "_ = load_dotenv(find_dotenv(), override=True)\n",
        "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
        "TIMESCALE_SERVICE_URL = os.environ[\"TIMESCALE_SERVICE_URL\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If running Google Colab use the code below (after setting the appropriate secrets)"
      ],
      "metadata": {
        "id": "E8r0BIp55Jj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "TIMESCALE_SERVICE_URL = userdata.get('TIMESCALE_SERVICE_URL')"
      ],
      "metadata": {
        "id": "lp2AHzfm5INe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhe5J4Lzl0Vk"
      },
      "source": [
        "## Extract content and metadata from git log JSON\n",
        "First lets load in the git log data into a new collection in our PostgreSQL database named `timescale_commits`\n",
        "\n",
        "You'll need to [download the sample dataset](https://s3.amazonaws.com/assets.timescale.com/ai/ts_git_log.json) and place it in the same directory as this notebook.\n",
        "\n",
        "You can use following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0wt9-OGbl0Vk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80b4d57-c434-432a-8d8e-345677758408"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2199k  100 2199k    0     0  3052k      0 --:--:-- --:--:-- --:--:-- 3055k\n"
          ]
        }
      ],
      "source": [
        "# Download the file using curl and save it as commit_history.csv\n",
        "# Note: Execute this command in your terminal, in the same directory as the notebook\n",
        "!curl -O \"https://s3.amazonaws.com/assets.timescale.com/ai/ts_git_log.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWG0Xuf5l0Vl"
      },
      "source": [
        "Next, let's create a set of functions to extract metadata from the git commits. Note, how we create the UUID based on the timestamp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P3wvpLVyl0Vl"
      },
      "outputs": [],
      "source": [
        "from timescale_vector import client\n",
        "from datetime import datetime\n",
        "\n",
        "def parse_date(date_string: str) -> datetime:\n",
        "    if date_string is None:\n",
        "        return None\n",
        "    time_format = \"%a %b %d %H:%M:%S %Y %z\"\n",
        "    return datetime.strptime(date_string, time_format)\n",
        "\n",
        "def split_name(input_string: str):\n",
        "    if input_string is None:\n",
        "        return None, None\n",
        "    start = input_string.find(\"<\")\n",
        "    end = input_string.find(\">\")\n",
        "    name = input_string[:start].strip()\n",
        "    email = input_string[start + 1 : end].strip()\n",
        "    return name, email\n",
        "\n",
        "def extract_metadata(record: dict, metadata: dict) -> dict:\n",
        "    dt = parse_date(record[\"date\"])\n",
        "    record_name, record_email = split_name(record[\"author\"])\n",
        "    # it is important to use a uuid v1 that contains the timestamp\n",
        "    metadata[\"id\"] = str(client.uuid_from_time(dt))\n",
        "    if dt is not None:\n",
        "        metadata[\"date\"] = dt.isoformat()\n",
        "    else:\n",
        "        metadata[\"date\"] = None\n",
        "    metadata[\"author\"] = record[\"author\"]\n",
        "    metadata[\"author_name\"] = record_name\n",
        "    metadata[\"author_email\"] = record_email\n",
        "    metadata[\"commit_hash\"] = record[\"commit\"]\n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oOxoX1Xl0Vl"
      },
      "source": [
        "Then, we load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b4b7fXtQl0Vl"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders.json_loader import JSONLoader\n",
        "\n",
        "# Define path to the JSON file relative to this notebook\n",
        "# Change this to the path to your JSON file\n",
        "FILE_PATH = \"ts_git_log.json\"\n",
        "\n",
        "# Load data from JSON file and extract metadata\n",
        "loader = JSONLoader(\n",
        "    file_path=FILE_PATH,\n",
        "    jq_schema=\".commit_history[]\",\n",
        "    text_content=False,\n",
        "    metadata_func=extract_metadata,\n",
        ")\n",
        "documents = loader.load()\n",
        "\n",
        "# Remove documents with None dates\n",
        "documents = [doc for doc in documents if doc.metadata[\"date\"] is not None]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOKDrJDcl0Vm"
      },
      "source": [
        "Since this is a demo, we will only load the first 500 records. In practice, you can load as many records as you want.\n",
        "\n",
        "We use the CharacterTextSplitter to split the documents into smaller chunks if needed for easier embedding. Note that this splitting process retains the metadata for each document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p47vJYMVl0Vm"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "NUM_RECORDS = 500\n",
        "documents = documents[:NUM_RECORDS]\n",
        "\n",
        "# Split the documents into chunks for embedding\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2xJ8Zznl0Vm"
      },
      "source": [
        "Let's see one document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OItiX5G4l0Vm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f517e7d-af85-4f5d-cc0f-afdae32e7805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='{\"commit\": \"44e41c12ab25e36c202f58e068ced262eadc8d16\", \"author\": \"Lakshmi Narayanan Sreethar<lakshmi@timescale.com>\", \"date\": \"Tue Sep 5 21:03:21 2023 +0530\", \"change summary\": \"Fix segfault in set_integer_now_func\", \"change details\": \"When an invalid function oid is passed to set_integer_now_func, it finds out that the function oid is invalid but before throwing the error, it calls ReleaseSysCache on an invalid tuple causing a segfault. Fixed that by removing the invalid call to ReleaseSysCache.  Fixes #6037 \"}' metadata={'source': '/content/ts_git_log.json', 'seq_num': 1, 'id': '8b407680-4c01-11ee-9cbc-86edc46a0120', 'date': '2023-09-05T21:03:21+05:30', 'author': 'Lakshmi Narayanan Sreethar<lakshmi@timescale.com>', 'author_name': 'Lakshmi Narayanan Sreethar', 'author_email': 'lakshmi@timescale.com', 'commit_hash': '44e41c12ab25e36c202f58e068ced262eadc8d16'}\n"
          ]
        }
      ],
      "source": [
        "print(documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyvVld4jl0Vm"
      },
      "source": [
        "## Load documents and metadata into TimescaleVector vectorstore\n",
        "Now that we have prepared our documents, let's process them and load them, along with their vector embedding representations into our Timescale Vector.\n",
        "\n",
        "First, we'll define a collection name, which will be the name of our table in the PostgreSQL database.\n",
        "\n",
        "We'll also define a time delta, which we pass to the `time_partition_interval` argument, which will be used to as the interval for partitioning the data by time. Each partition will consist of data for the specified length of time. We'll use 7 days for simplicity, but you can pick whatever value make sense for your use case -- for example if you query recent vectors frequently you might want to use a smaller time delta like 1 day, or if you query vectors over a decade long time period then you might want to use a larger time delta like 6 months or 1 year.\n",
        "\n",
        "Finally, we'll create the TimescaleVector instance. We specify the `ids` argument to be the `uuid` field in our metadata that we created in the pre-processing step above. We do this because we want the time part of our uuids to reflect dates in the past (i.e when the commit was made). However, if we wanted the current date and time to be associated with our document, we can remove the id argument and uuid's will be automatically created with the current date and time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "toew-DNol0Vm"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores.timescalevector import TimescaleVector\n",
        "from datetime import timedelta\n",
        "\n",
        "# Define collection name\n",
        "COLLECTION_NAME = \"timescale_commits\"\n",
        "embeddings = OpenAIEmbeddings(openai_api_key = OPENAI_API_KEY)\n",
        "\n",
        "# Create a Timescale Vector instance from the collection of documents\n",
        "db = TimescaleVector.from_documents(\n",
        "    embedding=embeddings,\n",
        "    ids=[doc.metadata[\"id\"] for doc in docs],\n",
        "    documents=docs,\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    service_url=TIMESCALE_SERVICE_URL,\n",
        "    time_partition_interval=timedelta(days=7),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhaG16Z4l0Vm"
      },
      "source": [
        "Create an index to speed up queries. Using the `create_index()` function without additional arguments will create a timescale_vector_index by default, using the default parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zk3T7dAhl0Vm"
      },
      "outputs": [],
      "source": [
        "# create an index\n",
        "# by default this will create a Timescale Vector (DiskANN) index\n",
        "db.create_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDwXOlqAl0Vm"
      },
      "source": [
        "## Querying vectors by time and similarity\n",
        "\n",
        "Now that we have loaded our documents into TimescaleVector, we can query them by time and similarity.\n",
        "\n",
        "TimescaleVector does this effeciently because any partition (7-day period in this example) that does not overlap with the query is completely excluded.\n",
        "\n",
        "TimescaleVector provides multiple methods for querying vectors by doing similarity search with time-based filtering.\n",
        "\n",
        "Let's take a look at each method below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "X6u7joaml0Vn"
      },
      "outputs": [],
      "source": [
        "# Time filter variables\n",
        "start_dt = datetime(2023, 8, 1, 22, 10, 35)  # Start date = 1 August 2023, 22:10:35\n",
        "end_dt = datetime(2023, 8, 30, 22, 10, 35)  # End date = 30 August 2023, 22:10:35\n",
        "td = timedelta(days=7)  # Time delta = 7 days\n",
        "\n",
        "query = \"What's new with TimescaleDB functions?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux47-gyAl0Vn"
      },
      "source": [
        "Method 1: Filter within a provided start date and end date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RiiwvRvhl0Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1354dcaa-61da-4fde-f17b-5991849c5992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Score:  0.17387111809939948\n",
            "Date:  2023-08-29T18:13:24+02:00\n",
            "{\"commit\": \" e4facda540286b0affba47ccc63959fefe2a7b26\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Tue Aug 29 18:13:24 2023 +0200\", \"change summary\": \"Add compatibility layer for _timescaledb_internal functions\", \"change details\": \"With timescaledb 2.12 all the functions present in _timescaledb_internal were moved into the _timescaledb_functions schema to improve schema security. This patch adds a compatibility layer so external callers of these internal functions will not break and allow for more flexibility when migrating. \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.18128876797623683\n",
            "Date:  2023-08-20T22:47:10+02:00\n",
            "{\"commit\": \" 0a66bdb8d36a1879246bd652e4c28500c4b951ab\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Sun Aug 20 22:47:10 2023 +0200\", \"change summary\": \"Move functions to _timescaledb_functions schema\", \"change details\": \"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - to_unix_microseconds(timestamptz) - to_timestamp(bigint) - to_timestamp_without_timezone(bigint) - to_date(bigint) - to_interval(bigint) - interval_to_usec(interval) - time_to_internal(anyelement) - subtract_integer_from_now(regclass, bigint) \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.1816416734946269\n",
            "Date:  2023-08-22T12:01:19+02:00\n",
            "{\"commit\": \" cf04496e4b4237440274eb25e4e02472fc4e06fc\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Tue Aug 22 12:01:19 2023 +0200\", \"change summary\": \"Move utility functions to _timescaledb_functions schema\", \"change details\": \"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - generate_uuid() - get_git_commit() - get_os_info() - tsl_loaded() \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.1845395785021221\n",
            "Date:  2023-08-09T15:26:03+03:00\n",
            "{\"commit\": \" 44eab9cf9bef34274c88efd37a750eaa74cd8044\", \"author\": \"Konstantina Skovola<konstantina@timescale.com>\", \"date\": \"Wed Aug 9 15:26:03 2023 +0300\", \"change summary\": \"Release 2.11.2\", \"change details\": \"This release contains bug fixes since the 2.11.1 release. We recommend that you upgrade at the next available opportunity.  **Features** * #5923 Feature flags for TimescaleDB features  **Bugfixes** * #5680 Fix DISTINCT query with JOIN on multiple segmentby columns * #5774 Fixed two bugs in decompression sorted merge code * #5786 Ensure pg_config --cppflags are passed * #5906 Fix quoting owners in sql scripts. * #5912 Fix crash in 1-step integer policy creation  **Thanks** * @mrksngl for submitting a PR to fix extension upgrade scripts * @ericdevries for reporting an issue with DISTINCT queries using segmentby columns of compressed hypertable \"}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Query for vectors between start_date and end_date\n",
        "docs_with_score = db.similarity_search_with_score(\n",
        "    query, start_date=start_dt, end_date=end_dt\n",
        ")\n",
        "\n",
        "for doc, score in docs_with_score:\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Score: \", score)\n",
        "    print(\"Date: \", doc.metadata[\"date\"])\n",
        "    print(doc.page_content)\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh18qJ6Ml0Vn"
      },
      "source": [
        "Method 2: Filter within a provided start date, and a time delta later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xxnvPXI7l0Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faebaf8a-98de-4d7d-9460-d99ca1cbfce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Score:  0.1848673312089063\n",
            "Date:  2023-08-03T14:30:23+03:00\n",
            "{\"commit\": \" 7aeed663b9c0f337b530fd6cad47704a51a9b2ec\", \"author\": \"Dmitry Simonenko<dmitry@timescale.com>\", \"date\": \"Thu Aug 3 14:30:23 2023 +0300\", \"change summary\": \"Feature flags for TimescaleDB features\", \"change details\": \"This PR adds several GUCs which allow to enable/disable major timescaledb features:  - enable_hypertable_create - enable_hypertable_compression - enable_cagg_create - enable_policy_create \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.20524346828460693\n",
            "Date:  2023-08-07T18:31:40+02:00\n",
            "{\"commit\": \" 07762ea4cedefc88497f0d1f8712d1515cdc5b6e\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Mon Aug 7 18:31:40 2023 +0200\", \"change summary\": \"Test timescaledb debian 12 packages in CI\", \"change details\": \"\"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.21125440865857947\n",
            "Date:  2023-08-03T14:36:39+03:00\n",
            "{\"commit\": \" 2863daf3df83c63ee36c0cf7b66c522da5b4e127\", \"author\": \"Dmitry Simonenko<dmitry@timescale.com>\", \"date\": \"Thu Aug 3 14:36:39 2023 +0300\", \"change summary\": \"Support CREATE INDEX ONLY ON main table\", \"change details\": \"This PR adds support for CREATE INDEX ONLY ON clause which allows to create index only on the main table excluding chunks.  Fix #5908 \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.21730369329452515\n",
            "Date:  2023-08-02T20:24:14+01:00\n",
            "{\"commit\": \" 3af0d282ea71d9a8f27159a6171e9516e62ec9cb\", \"author\": \"Lakshmi Narayanan Sreethar<lakshmi@timescale.com>\", \"date\": \"Wed Aug 2 20:24:14 2023 +0100\", \"change summary\": \"PG16: ExecInsertIndexTuples requires additional parameter\", \"change details\": \"PG16 adds a new boolean parameter to the ExecInsertIndexTuples function to denote if the index is a BRIN index, which is then used to determine if the index update can be skipped. The fix also removes the INDEX_ATTR_BITMAP_ALL enum value.  Adapt these changes by updating the compat function to accomodate the new parameter added to the ExecInsertIndexTuples function and using an alternative for the removed INDEX_ATTR_BITMAP_ALL enum value.  postgres/postgres@19d8e23 \"}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Method 2: Query for vectors between start_dt and a time delta td later\n",
        "# Most relevant vectors between 1 August and 7 days later\n",
        "docs_with_score = db.similarity_search_with_score(\n",
        "    query, start_date=start_dt, time_delta=td\n",
        ")\n",
        "\n",
        "for doc, score in docs_with_score:\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Score: \", score)\n",
        "    print(\"Date: \", doc.metadata[\"date\"])\n",
        "    print(doc.page_content)\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4nzlXFtl0Vn"
      },
      "source": [
        "Method 3: Filter within a provided end date and a time delta earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nisxw2cHl0Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90c67932-843a-495c-f170-1e65c47b5421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Score:  0.17387111809939948\n",
            "Date:  2023-08-29T18:13:24+02:00\n",
            "{\"commit\": \" e4facda540286b0affba47ccc63959fefe2a7b26\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Tue Aug 29 18:13:24 2023 +0200\", \"change summary\": \"Add compatibility layer for _timescaledb_internal functions\", \"change details\": \"With timescaledb 2.12 all the functions present in _timescaledb_internal were moved into the _timescaledb_functions schema to improve schema security. This patch adds a compatibility layer so external callers of these internal functions will not break and allow for more flexibility when migrating. \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.18496502548247584\n",
            "Date:  2023-08-29T10:49:47+02:00\n",
            "{\"commit\": \" a9751ccd5eb030026d7b975d22753f5964972389\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Tue Aug 29 10:49:47 2023 +0200\", \"change summary\": \"Move partitioning functions to _timescaledb_functions schema\", \"change details\": \"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - get_partition_for_key(val anyelement) - get_partition_hash(val anyelement) \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.187219500541687\n",
            "Date:  2023-08-28T23:26:23+02:00\n",
            "{\"commit\": \" b2a91494a11d8b82849b6f11f9ea6dc26ef8a8cb\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Mon Aug 28 23:26:23 2023 +0200\", \"change summary\": \"Move ddl_internal functions to _timescaledb_functions schema\", \"change details\": \"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - chunk_constraint_add_table_constraint(_timescaledb_catalog.chunk_constraint) - chunk_drop_replica(regclass,name) - chunk_index_clone(oid) - chunk_index_replace(oid,oid) - create_chunk_replica_table(regclass,name) - drop_stale_chunks(name,integer[]) - health() - hypertable_constraint_add_table_fk_constraint(name,name,name,integer) - process_ddl_event() - wait_subscription_sync(name,name,integer,numeric) \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.1882814877200999\n",
            "Date:  2023-08-27T13:20:04+02:00\n",
            "{\"commit\": \" e02b1f348eb4c48def00b7d5227238b4d9d41a4a\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Sun Aug 27 13:20:04 2023 +0200\", \"change summary\": \"Simplify schema move update script\", \"change details\": \"Use dynamic sql to create the ALTER FUNCTION statements for those functions that may not exist in previous versions. \"}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Method 3: Query for vectors between end_dt and a time delta td earlier\n",
        "# Most relevant vectors between 30 August and 7 days earlier\n",
        "docs_with_score = db.similarity_search_with_score(query, end_date=end_dt, time_delta=td)\n",
        "\n",
        "for doc, score in docs_with_score:\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Score: \", score)\n",
        "    print(\"Date: \", doc.metadata[\"date\"])\n",
        "    print(doc.page_content)\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4jq0QIFl0Vn"
      },
      "source": [
        "## Querying using other metadata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX5JZWXSl0Vn"
      },
      "source": [
        "You can also filter based on other metadata.  This shows a similarity search with a filter on author name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LCxnk7GRl0Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d78a7d3-4304-4a2d-9b26-d1376b3ef287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Score:  0.16742825508117676\n",
            "Date:  2023-04-11T22:01:14+02:00\n",
            "Author:  Sven Klemm\n",
            "{\"commit\": \" 0595ff0888f2ffb8d313acb0bda9642578a9ade3\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Tue Apr 11 22:01:14 2023 +0200\", \"change summary\": \"Move type support functions into _timescaledb_functions schema\", \"change details\": \"\"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.17063194513320923\n",
            "Date:  2023-04-06T13:00:00+02:00\n",
            "Author:  Sven Klemm\n",
            "{\"commit\": \" 04f43335dea11e9c467ee558ad8edfc00c1a45ed\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Thu Apr 6 13:00:00 2023 +0200\", \"change summary\": \"Move aggregate support function into _timescaledb_functions\", \"change details\": \"This patch moves the support functions for histogram, first and last into the _timescaledb_functions schema. Since we alter the schema of the existing functions in upgrade scripts and do not change the aggregates this should work completely transparently for any user objects using those aggregates. \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.17387111809939948\n",
            "Date:  2023-08-29T18:13:24+02:00\n",
            "Author:  Sven Klemm\n",
            "{\"commit\": \" e4facda540286b0affba47ccc63959fefe2a7b26\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Tue Aug 29 18:13:24 2023 +0200\", \"change summary\": \"Add compatibility layer for _timescaledb_internal functions\", \"change details\": \"With timescaledb 2.12 all the functions present in _timescaledb_internal were moved into the _timescaledb_functions schema to improve schema security. This patch adds a compatibility layer so external callers of these internal functions will not break and allow for more flexibility when migrating. \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.1747262061294783\n",
            "Date:  2023-03-31T08:22:57+02:00\n",
            "Author:  Sven Klemm\n",
            "{\"commit\": \" feef9206facc5c5f506661de4a81d96ef059b095\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Fri Mar 31 08:22:57 2023 +0200\", \"change summary\": \"Add _timescaledb_functions schema\", \"change details\": \"Currently internal user objects like chunks and our functions live in the same schema making locking down that schema hard. This patch adds a new schema _timescaledb_functions that is meant to be the schema used for timescaledb internal functions to allow separation of code and chunks or other user objects. \"}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "docs_with_score = db.similarity_search_with_score(\n",
        "    query,filter={\"author_name\": \"Sven Klemm\"}\n",
        ")\n",
        "\n",
        "for doc, score in docs_with_score:\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Score: \", score)\n",
        "    print(\"Date: \", doc.metadata[\"date\"])\n",
        "    print(\"Author: \", doc.metadata[\"author_name\"])\n",
        "    print(doc.page_content)\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XvnZc8Tl0Vn"
      },
      "source": [
        "You can also combine it with time-based search. This shows a similarity search with a time filter as well as a filter on author name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "W29XXScKl0Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d2db4f9-8bba-4cf0-fdd5-c428d9344c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Score:  0.17387111809939948\n",
            "Date:  2023-08-29T18:13:24+02:00\n",
            "Author:  Sven Klemm\n",
            "{\"commit\": \" e4facda540286b0affba47ccc63959fefe2a7b26\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Tue Aug 29 18:13:24 2023 +0200\", \"change summary\": \"Add compatibility layer for _timescaledb_internal functions\", \"change details\": \"With timescaledb 2.12 all the functions present in _timescaledb_internal were moved into the _timescaledb_functions schema to improve schema security. This patch adds a compatibility layer so external callers of these internal functions will not break and allow for more flexibility when migrating. \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.18128876797623683\n",
            "Date:  2023-08-20T22:47:10+02:00\n",
            "Author:  Sven Klemm\n",
            "{\"commit\": \" 0a66bdb8d36a1879246bd652e4c28500c4b951ab\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Sun Aug 20 22:47:10 2023 +0200\", \"change summary\": \"Move functions to _timescaledb_functions schema\", \"change details\": \"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - to_unix_microseconds(timestamptz) - to_timestamp(bigint) - to_timestamp_without_timezone(bigint) - to_date(bigint) - to_interval(bigint) - interval_to_usec(interval) - time_to_internal(anyelement) - subtract_integer_from_now(regclass, bigint) \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.1816416734946269\n",
            "Date:  2023-08-22T12:01:19+02:00\n",
            "Author:  Sven Klemm\n",
            "{\"commit\": \" cf04496e4b4237440274eb25e4e02472fc4e06fc\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Tue Aug 22 12:01:19 2023 +0200\", \"change summary\": \"Move utility functions to _timescaledb_functions schema\", \"change details\": \"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - generate_uuid() - get_git_commit() - get_os_info() - tsl_loaded() \"}\n",
            "--------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------\n",
            "Score:  0.18496502548247584\n",
            "Date:  2023-08-29T10:49:47+02:00\n",
            "Author:  Sven Klemm\n",
            "{\"commit\": \" a9751ccd5eb030026d7b975d22753f5964972389\", \"author\": \"Sven Klemm<sven@timescale.com>\", \"date\": \"Tue Aug 29 10:49:47 2023 +0200\", \"change summary\": \"Move partitioning functions to _timescaledb_functions schema\", \"change details\": \"To increase schema security we do not want to mix our own internal objects with user objects. Since chunks are created in the _timescaledb_internal schema our internal functions should live in a different dedicated schema. This patch make the necessary adjustments for the following functions:  - get_partition_for_key(val anyelement) - get_partition_hash(val anyelement) \"}\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "docs_with_score = db.similarity_search_with_score(\n",
        "    query, start_date=start_dt, end_date=end_dt, filter={\"author_name\": \"Sven Klemm\"}\n",
        ")\n",
        "\n",
        "for doc, score in docs_with_score:\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Score: \", score)\n",
        "    print(\"Date: \", doc.metadata[\"date\"])\n",
        "    print(\"Author: \", doc.metadata[\"author_name\"])\n",
        "    print(doc.page_content)\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0uQPCT3l0Vn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nbdev_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}