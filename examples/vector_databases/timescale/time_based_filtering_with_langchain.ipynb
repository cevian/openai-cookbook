{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search with time-based filtering using LangChain\n",
    "\n",
    "A key use case for Timescale Vector is efficient time-based vector search. Timescale Vector enables this by automatically partitioning vectors (and associated metadata) by time. This allows you to efficiently query vectors by both similarity to a query vector and time because we can exclude entire partitions that don't overlap with the query time.\n",
    "\n",
    "Time-based vector search functionality is helpful for applications like:\n",
    "- Storing and retrieving LLM response history (e.g. chatbots)\n",
    "- Finding the most recent embeddings that are similar to a query vector (e.g recent news).\n",
    "- Constraining similarity search to a relevant time range (e.g asking time-based questions about a knowledge base)\n",
    "\n",
    "To illustrate how to use TimescaleVector's time-based vector search functionality, we'll ask questions about the git log history for TimescaleDB . We'll illustrate how to add documents with a time-based uuid and how run similarity searches with time range filters.\n",
    "\n",
    "## Setup your environment\n",
    "\n",
    "First, install the necessary prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pip install necessary packages\n",
    "%pip install timescale-vector\n",
    "%pip install openai\n",
    "%pip install langchain\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, setup your secrets. You'll need an API key from [OpenAI](https://platform.openai.com/) and a service url from [Timescale](https://console.cloud.timescale.com/signup?utm_campaign=vectorlaunch&utm_source=aicookbooks&utm_medium=referral). For security, we suggest storing these in a dotenv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get openAI api key by reading local .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv(), override=True)\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "TIMESCALE_SERVICE_URL = os.environ[\"TIMESCALE_SERVICE_URL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract content and metadata from git log JSON\n",
    "First lets load in the git log data into a new collection in our PostgreSQL database named `timescale_commits`\n",
    "\n",
    "You'll need to [download the sample dataset](https://s3.amazonaws.com/assets.timescale.com/ai/ts_git_log.json) and place it in the same directory as this notebook.\n",
    "\n",
    "You can use following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file using curl and save it as commit_history.csv\n",
    "# Note: Execute this command in your terminal, in the same directory as the notebook\n",
    "!curl -O \"https://s3.amazonaws.com/assets.timescale.com/ai/ts_git_log.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a set of functions to extract metadata from the git commits. Note, how we create the UUID based on the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timescale_vector import client\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_date(date_string: str) -> datetime:\n",
    "    if date_string is None:\n",
    "        return None\n",
    "    time_format = \"%a %b %d %H:%M:%S %Y %z\"\n",
    "    return datetime.strptime(date_string, time_format)\n",
    "\n",
    "def split_name(input_string: str):\n",
    "    if input_string is None:\n",
    "        return None, None\n",
    "    start = input_string.find(\"<\")\n",
    "    end = input_string.find(\">\")\n",
    "    name = input_string[:start].strip()\n",
    "    email = input_string[start + 1 : end].strip()\n",
    "    return name, email\n",
    "\n",
    "def extract_metadata(record: dict, metadata: dict) -> dict:\n",
    "    dt = parse_date(record[\"date\"])\n",
    "    record_name, record_email = split_name(record[\"author\"])\n",
    "    # it is important to use a uuid v1 that contains the timestamp\n",
    "    metadata[\"id\"] = str(client.uuid_from_time(dt))\n",
    "    if dt is not None:\n",
    "        metadata[\"date\"] = dt.isoformat()\n",
    "    else:\n",
    "        metadata[\"date\"] = None\n",
    "    metadata[\"author\"] = record[\"author\"]\n",
    "    metadata[\"author_name\"] = record_name\n",
    "    metadata[\"author_email\"] = record_email\n",
    "    metadata[\"commit_hash\"] = record[\"commit\"]\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we loade the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.json_loader import JSONLoader\n",
    "\n",
    "# Define path to the JSON file relative to this notebook\n",
    "# Change this to the path to your JSON file\n",
    "FILE_PATH = \"ts_git_log.json\"\n",
    "\n",
    "# Load data from JSON file and extract metadata\n",
    "loader = JSONLoader(\n",
    "    file_path=FILE_PATH,\n",
    "    jq_schema=\".commit_history[]\",\n",
    "    text_content=False,\n",
    "    metadata_func=extract_metadata,\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "# Remove documents with None dates\n",
    "documents = [doc for doc in documents if doc.metadata[\"date\"] is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a demo, we will only load the first 500 records. In practice, you can load as many records as you want.\n",
    "\n",
    "We use the CharacterTextSplitter to split the documents into smaller chunks if needed for easier embedding. Note that this splitting process retains the metadata for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "NUM_RECORDS = 500\n",
    "documents = documents[:NUM_RECORDS]\n",
    "\n",
    "# Split the documents into chunks for embedding\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see one document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents and metadata into TimescaleVector vectorstore\n",
    "Now that we have prepared our documents, let's process them and load them, along with their vector embedding representations into our Timescale Vector.\n",
    "\n",
    "First, we'll define a collection name, which will be the name of our table in the PostgreSQL database. \n",
    "\n",
    "We'll also define a time delta, which we pass to the `time_partition_interval` argument, which will be used to as the interval for partitioning the data by time. Each partition will consist of data for the specified length of time. We'll use 7 days for simplicity, but you can pick whatever value make sense for your use case -- for example if you query recent vectors frequently you might want to use a smaller time delta like 1 day, or if you query vectors over a decade long time period then you might want to use a larger time delta like 6 months or 1 year.\n",
    "\n",
    "Finally, we'll create the TimescaleVector instance. We specify the `ids` argument to be the `uuid` field in our metadata that we created in the pre-processing step above. We do this because we want the time part of our uuids to reflect dates in the past (i.e when the commit was made). However, if we wanted the current date and time to be associated with our document, we can remove the id argument and uuid's will be automatically created with the current date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores.timescalevector import TimescaleVector\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define collection name\n",
    "COLLECTION_NAME = \"timescale_commits\"\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create a Timescale Vector instance from the collection of documents\n",
    "db = TimescaleVector.from_documents(\n",
    "    embedding=embeddings,\n",
    "    ids=[doc.metadata[\"id\"] for doc in docs],\n",
    "    documents=docs,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    service_url=TIMESCALE_SERVICE_URL,\n",
    "    time_partition_interval=timedelta(days=7),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an index to speed up queries. Using the `create_index()` function without additional arguments will create a timescale_vector_index by default, using the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index\n",
    "# by default this will create a Timescale Vector (DiskANN) index\n",
    "db.create_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying vectors by time and similarity\n",
    "\n",
    "Now that we have loaded our documents into TimescaleVector, we can query them by time and similarity.\n",
    "\n",
    "TimescaleVector does this effeciently because any partition (7-day period in this example) that does not overlap with the query is completely excluded.\n",
    "\n",
    "TimescaleVector provides multiple methods for querying vectors by doing similarity search with time-based filtering. \n",
    "\n",
    "Let's take a look at each method below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time filter variables\n",
    "start_dt = datetime(2023, 8, 1, 22, 10, 35)  # Start date = 1 August 2023, 22:10:35\n",
    "end_dt = datetime(2023, 8, 30, 22, 10, 35)  # End date = 30 August 2023, 22:10:35\n",
    "td = timedelta(days=7)  # Time delta = 7 days\n",
    "\n",
    "query = \"What's new with TimescaleDB functions?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Filter within a provided start date and end date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Query for vectors between start_date and end_date\n",
    "docs_with_score = db.similarity_search_with_score(\n",
    "    query, start_date=start_dt, end_date=end_dt\n",
    ")\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(\"Date: \", doc.metadata[\"date\"])\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Filter within a provided start date, and a time delta later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Query for vectors between start_dt and a time delta td later\n",
    "# Most relevant vectors between 1 August and 7 days later\n",
    "docs_with_score = db.similarity_search_with_score(\n",
    "    query, start_date=start_dt, time_delta=td\n",
    ")\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(\"Date: \", doc.metadata[\"date\"])\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 3: Filter within a provided end date and a time delta earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Query for vectors between end_dt and a time delta td earlier\n",
    "# Most relevant vectors between 30 August and 7 days earlier\n",
    "docs_with_score = db.similarity_search_with_score(query, end_date=end_dt, time_delta=td)\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(\"Date: \", doc.metadata[\"date\"])\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying using other metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also filter based on other metadata.  This shows a similarity search with a filter on author name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_with_score = db.similarity_search_with_score(\n",
    "    query,filter={\"author_name\": \"Sven Klemm\"}\n",
    ")\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(\"Date: \", doc.metadata[\"date\"])\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also combine it with time-based search. This shows a similarity search with a time filter as well as a filter on author name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_with_score = db.similarity_search_with_score(\n",
    "    query, start_date=start_dt, end_date=end_dt, filter={\"author_name\": \"Sven Klemm\"}\n",
    ")\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(\"Date: \", doc.metadata[\"date\"])\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
